    1: from itertools import chain
    1: import re
    1: import warnings
       
    1: from xml.sax.saxutils import unescape
       
    1: from bleach import html5lib_shim
    1: from bleach import parse_shim
       
       
       #: Set of allowed tags
    2: ALLOWED_TAGS = frozenset(
    1:     (
               "a",
               "abbr",
               "acronym",
               "b",
               "blockquote",
               "code",
               "em",
               "i",
               "li",
               "ol",
               "strong",
               "ul",
           )
       )
       
       
       #: Map of allowed attributes by tag
    1: ALLOWED_ATTRIBUTES = {
    1:     "a": ["href", "title"],
    1:     "abbr": ["title"],
    1:     "acronym": ["title"],
       }
       
       #: List of allowed protocols
    1: ALLOWED_PROTOCOLS = frozenset(("http", "https", "mailto"))
       
       #: Invisible characters--0 to and including 31 except 9 (tab), 10 (lf), and 13 (cr)
    2: INVISIBLE_CHARACTERS = "".join(
   31:     [chr(c) for c in chain(range(0, 9), range(11, 13), range(14, 32))]
       )
       
       #: Regexp for characters that are invisible
    1: INVISIBLE_CHARACTERS_RE = re.compile("[" + INVISIBLE_CHARACTERS + "]", re.UNICODE)
       
       #: String to replace invisible characters with. This can be a character, a
       #: string, or even a function that takes a Python re matchobj
    1: INVISIBLE_REPLACEMENT_CHAR = "?"
       
       
    2: class NoCssSanitizerWarning(UserWarning):
    1:     pass
       
       
    2: class Cleaner:
    1:     """Cleaner for cleaning HTML fragments of malicious content
       
           This cleaner is a security-focused function whose sole purpose is to remove
           malicious content from a string such that it can be displayed as content in
           a web page.
       
           To use::
       
               from bleach.sanitizer import Cleaner
       
               cleaner = Cleaner()
       
               for text in all_the_yucky_things:
                   sanitized = cleaner.clean(text)
       
           .. Note::
       
              This cleaner is not designed to use to transform content to be used in
              non-web-page contexts.
       
           .. Warning::
       
              This cleaner is not thread-safe--the html parser has internal state.
              Create a separate cleaner per thread!
       
       
           """
       
    1:     def __init__(
               self,
    1:         tags=ALLOWED_TAGS,
    1:         attributes=ALLOWED_ATTRIBUTES,
    1:         protocols=ALLOWED_PROTOCOLS,
    1:         strip=False,
    1:         strip_comments=True,
    1:         filters=None,
    1:         css_sanitizer=None,
           ):
               """Initializes a Cleaner
       
               :arg set tags: set of allowed tags; defaults to
                   ``bleach.sanitizer.ALLOWED_TAGS``
       
               :arg dict attributes: allowed attributes; can be a callable, list or dict;
                   defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``
       
               :arg list protocols: allowed list of protocols for links; defaults
                   to ``bleach.sanitizer.ALLOWED_PROTOCOLS``
       
               :arg bool strip: whether or not to strip disallowed elements
       
               :arg bool strip_comments: whether or not to strip HTML comments
       
               :arg list filters: list of html5lib Filter classes to pass streamed content through
       
                   .. seealso:: http://html5lib.readthedocs.io/en/latest/movingparts.html#filters
       
                   .. Warning::
       
                      Using filters changes the output of ``bleach.Cleaner.clean``.
                      Make sure the way the filters change the output are secure.
       
               :arg CSSSanitizer css_sanitizer: instance with a "sanitize_css" method for
                   sanitizing style attribute values and style text; defaults to None
       
               """
    5:         self.tags = tags
    5:         self.attributes = attributes
    5:         self.protocols = protocols
    5:         self.strip = strip
    5:         self.strip_comments = strip_comments
    5:         self.filters = filters or []
    5:         self.css_sanitizer = css_sanitizer
       
   10:         self.parser = html5lib_shim.BleachHTMLParser(
    5:             tags=self.tags,
    5:             strip=self.strip,
    5:             consume_entities=False,
    5:             namespaceHTMLElements=False,
               )
    5:         self.walker = html5lib_shim.getTreeWalker("etree")
   10:         self.serializer = html5lib_shim.BleachHTMLSerializer(
    5:             quote_attr_values="always",
    5:             omit_optional_tags=False,
    5:             escape_lt_in_attrs=True,
                   # We want to leave entities as they are without escaping or
                   # resolving or expanding
    5:             resolve_entities=False,
                   # Bleach has its own sanitizer, so don't use the html5lib one
    5:             sanitize=False,
                   # clean preserves attr order
    5:             alphabetical_attributes=False,
               )
       
    5:         if css_sanitizer is None:
                   # FIXME(willkg): this doesn't handle when attributes or an
                   # attributes value is a callable
    5:             attributes_values = []
    5:             if isinstance(attributes, list):
>>>>>>                 attributes_values = attributes
       
    5:             elif isinstance(attributes, dict):
    5:                 attributes_values = []
   20:                 for values in attributes.values():
   15:                     if isinstance(values, (list, tuple)):
   15:                         attributes_values.extend(values)
       
    5:             if "style" in attributes_values:
>>>>>>                 warnings.warn(
>>>>>>                     "'style' attribute specified, but css_sanitizer not set.",
>>>>>>                     category=NoCssSanitizerWarning,
                       )
       
    1:     def clean(self, text):
               """Cleans text and returns sanitized result as unicode
       
               :arg str text: text to be cleaned
       
               :returns: sanitized text as unicode
       
               :raises TypeError: if ``text`` is not a text type
       
               """
    5:         if not isinstance(text, str):
>>>>>>             message = (
>>>>>>                 f"argument cannot be of {text.__class__.__name__!r} type, "
>>>>>>                 + "must be of text type"
                   )
>>>>>>             raise TypeError(message)
       
    5:         if not text:
>>>>>>             return ""
       
    5:         dom = self.parser.parseFragment(text)
   10:         filtered = BleachSanitizerFilter(
    5:             source=self.walker(dom),
    5:             allowed_tags=self.tags,
    5:             attributes=self.attributes,
    5:             strip_disallowed_tags=self.strip,
    5:             strip_html_comments=self.strip_comments,
    5:             css_sanitizer=self.css_sanitizer,
    5:             allowed_protocols=self.protocols,
               )
       
               # Apply any filters after the BleachSanitizerFilter
    5:         for filter_class in self.filters:
>>>>>>             filtered = filter_class(source=filtered)
       
    5:         return self.serializer.render(filtered)
       
       
    1: def attribute_filter_factory(attributes):
           """Generates attribute filter function for the given attributes value
       
           The attributes value can take one of several shapes. This returns a filter
           function appropriate to the attributes value. One nice thing about this is
           that there's less if/then shenanigans in the ``allow_token`` method.
       
           """
    5:     if callable(attributes):
>>>>>>         return attributes
       
    5:     if isinstance(attributes, dict):
       
    5:         def _attr_filter(tag, attr, value):
>>>>>>             if tag in attributes:
>>>>>>                 attr_val = attributes[tag]
>>>>>>                 if callable(attr_val):
>>>>>>                     return attr_val(tag, attr, value)
       
>>>>>>                 if attr in attr_val:
>>>>>>                     return True
       
>>>>>>             if "*" in attributes:
>>>>>>                 attr_val = attributes["*"]
>>>>>>                 if callable(attr_val):
>>>>>>                     return attr_val(tag, attr, value)
       
>>>>>>                 return attr in attr_val
       
>>>>>>             return False
       
    5:         return _attr_filter
       
>>>>>>     if isinstance(attributes, list):
       
>>>>>>         def _attr_filter(tag, attr, value):
>>>>>>             return attr in attributes
       
>>>>>>         return _attr_filter
       
>>>>>>     raise ValueError("attributes needs to be a callable, a list or a dict")
       
       
    2: class BleachSanitizerFilter(html5lib_shim.SanitizerFilter):
    1:     """html5lib Filter that sanitizes text
       
           This filter can be used anywhere html5lib filters can be used.
       
           """
       
    1:     def __init__(
               self,
               source,
    1:         allowed_tags=ALLOWED_TAGS,
    1:         attributes=ALLOWED_ATTRIBUTES,
    1:         allowed_protocols=ALLOWED_PROTOCOLS,
    1:         attr_val_is_uri=html5lib_shim.attr_val_is_uri,
    1:         svg_attr_val_allows_ref=html5lib_shim.svg_attr_val_allows_ref,
    1:         svg_allow_local_href=html5lib_shim.svg_allow_local_href,
    1:         strip_disallowed_tags=False,
    1:         strip_html_comments=True,
    1:         css_sanitizer=None,
           ):
               """Creates a BleachSanitizerFilter instance
       
               :arg source: html5lib TreeWalker stream as an html5lib TreeWalker
       
               :arg set allowed_tags: set of allowed tags; defaults to
                   ``bleach.sanitizer.ALLOWED_TAGS``
       
               :arg dict attributes: allowed attributes; can be a callable, list or dict;
                   defaults to ``bleach.sanitizer.ALLOWED_ATTRIBUTES``
       
               :arg list allowed_protocols: allowed list of protocols for links; defaults
                   to ``bleach.sanitizer.ALLOWED_PROTOCOLS``
       
               :arg attr_val_is_uri: set of attributes that have URI values
       
               :arg svg_attr_val_allows_ref: set of SVG attributes that can have
                   references
       
               :arg svg_allow_local_href: set of SVG elements that can have local
                   hrefs
       
               :arg bool strip_disallowed_tags: whether or not to strip disallowed
                   tags
       
               :arg bool strip_html_comments: whether or not to strip HTML comments
       
               :arg CSSSanitizer css_sanitizer: instance with a "sanitize_css" method for
                   sanitizing style attribute values and style text; defaults to None
       
               """
               # NOTE(willkg): This is the superclass of
               # html5lib.filters.sanitizer.Filter. We call this directly skipping the
               # __init__ for html5lib.filters.sanitizer.Filter because that does
               # things we don't need to do and kicks up the deprecation warning for
               # using Sanitizer.
    5:         html5lib_shim.Filter.__init__(self, source)
       
    5:         self.allowed_tags = frozenset(allowed_tags)
    5:         self.allowed_protocols = frozenset(allowed_protocols)
       
    5:         self.attr_filter = attribute_filter_factory(attributes)
    5:         self.strip_disallowed_tags = strip_disallowed_tags
    5:         self.strip_html_comments = strip_html_comments
       
    5:         self.attr_val_is_uri = attr_val_is_uri
    5:         self.svg_attr_val_allows_ref = svg_attr_val_allows_ref
    5:         self.css_sanitizer = css_sanitizer
    5:         self.svg_allow_local_href = svg_allow_local_href
       
    1:     def sanitize_stream(self, token_iterator):
   10:         for token in token_iterator:
    5:             ret = self.sanitize_token(token)
       
    5:             if not ret:
>>>>>>                 continue
       
    5:             if isinstance(ret, list):
>>>>>>                 yield from ret
                   else:
    5:                 yield ret
       
    1:     def merge_characters(self, token_iterator):
               """Merge consecutive Characters tokens in a stream"""
    5:         characters_buffer = []
       
   10:         for token in token_iterator:
    5:             if characters_buffer:
>>>>>>                 if token["type"] == "Characters":
>>>>>>                     characters_buffer.append(token)
>>>>>>                     continue
                       else:
                           # Merge all the characters tokens together into one and then
                           # operate on it.
>>>>>>                     new_token = {
>>>>>>                         "data": "".join(
>>>>>>                             [char_token["data"] for char_token in characters_buffer]
                               ),
>>>>>>                         "type": "Characters",
                           }
>>>>>>                     characters_buffer = []
>>>>>>                     yield new_token
       
    5:             elif token["type"] == "Characters":
    5:                 characters_buffer.append(token)
    5:                 continue
       
>>>>>>             yield token
       
    5:         new_token = {
   15:             "data": "".join([char_token["data"] for char_token in characters_buffer]),
    5:             "type": "Characters",
               }
    5:         yield new_token
       
    1:     def __iter__(self):
   10:         return self.merge_characters(
    5:             self.sanitize_stream(html5lib_shim.Filter.__iter__(self))
               )
       
    1:     def sanitize_token(self, token):
               """Sanitize a token either by HTML-encoding or dropping.
       
               Unlike sanitizer.Filter, allowed_attributes can be a dict of {'tag':
               ['attribute', 'pairs'], 'tag': callable}.
       
               Here callable is a function with two arguments of attribute name and
               value. It should return true of false.
       
               Also gives the option to strip tags instead of encoding.
       
               :arg dict token: token to sanitize
       
               :returns: token or list of tokens
       
               """
    5:         token_type = token["type"]
    5:         if token_type in ["StartTag", "EndTag", "EmptyTag"]:
>>>>>>             if token["name"] in self.allowed_tags:
>>>>>>                 return self.allow_token(token)
       
>>>>>>             elif self.strip_disallowed_tags:
>>>>>>                 return None
       
                   else:
>>>>>>                 return self.disallowed_token(token)
       
    5:         elif token_type == "Comment":
>>>>>>             if not self.strip_html_comments:
                       # call lxml.sax.saxutils to escape &, <, and > in addition to " and '
>>>>>>                 token["data"] = html5lib_shim.escape(
>>>>>>                     token["data"], entities={'"': "&quot;", "'": "&#x27;"}
                       )
>>>>>>                 return token
                   else:
>>>>>>                 return None
       
    5:         elif token_type == "Characters":
    5:             return self.sanitize_characters(token)
       
               else:
>>>>>>             return token
       
    1:     def sanitize_characters(self, token):
               """Handles Characters tokens
       
               Our overridden tokenizer doesn't do anything with entities. However,
               that means that the serializer will convert all ``&`` in Characters
               tokens to ``&amp;``.
       
               Since we don't want that, we extract entities here and convert them to
               Entity tokens so the serializer will let them be.
       
               :arg token: the Characters token to work on
       
               :returns: a list of tokens
       
               """
    5:         data = token.get("data", "")
       
    5:         if not data:
>>>>>>             return token
       
    5:         data = INVISIBLE_CHARACTERS_RE.sub(INVISIBLE_REPLACEMENT_CHAR, data)
    5:         token["data"] = data
       
               # If there isn't a & in the data, we can return now
    5:         if "&" not in data:
    5:             return token
       
>>>>>>         new_tokens = []
       
               # For each possible entity that starts with a "&", we try to extract an
               # actual entity and re-tokenize accordingly
>>>>>>         for part in html5lib_shim.next_possible_entity(data):
>>>>>>             if not part:
>>>>>>                 continue
       
>>>>>>             if part.startswith("&"):
>>>>>>                 entity = html5lib_shim.match_entity(part)
>>>>>>                 if entity is not None:
>>>>>>                     if entity == "amp":
                               # LinkifyFilter can't match urls across token boundaries
                               # which is problematic with &amp; since that shows up in
                               # querystrings all the time. This special-cases &amp;
                               # and converts it to a & and sticks it in as a
                               # Characters token. It'll get merged with surrounding
                               # tokens in the BleachSanitizerfilter.__iter__ and
                               # escaped in the serializer.
>>>>>>                         new_tokens.append({"type": "Characters", "data": "&"})
                           else:
>>>>>>                         new_tokens.append({"type": "Entity", "name": entity})
       
                           # Length of the entity plus 2--one for & at the beginning
                           # and one for ; at the end
>>>>>>                     remainder = part[len(entity) + 2 :]
>>>>>>                     if remainder:
>>>>>>                         new_tokens.append({"type": "Characters", "data": remainder})
>>>>>>                     continue
       
>>>>>>             new_tokens.append({"type": "Characters", "data": part})
       
>>>>>>         return new_tokens
       
    1:     def sanitize_uri_value(self, value, allowed_protocols):
               """Checks a uri value to see if it's allowed
       
               :arg value: the uri value to sanitize
               :arg allowed_protocols: list of allowed protocols
       
               :returns: allowed value or None
       
               """
               # NOTE(willkg): This transforms the value into a normalized one that's
               # easier to match and verify, but shouldn't get returned since it's
               # vastly different than the original value.
       
               # Convert all character entities in the value
>>>>>>         normalized_uri = html5lib_shim.convert_entities(value)
       
               # Nix backtick, space characters, and control characters
>>>>>>         normalized_uri = re.sub(r"[`\000-\040\177-\240\s]+", "", normalized_uri)
       
               # Remove REPLACEMENT characters
>>>>>>         normalized_uri = normalized_uri.replace("\ufffd", "")
       
               # Lowercase it--this breaks the value, but makes it easier to match
               # against
>>>>>>         normalized_uri = normalized_uri.lower()
       
>>>>>>         try:
                   # Drop attributes with uri values that have protocols that aren't
                   # allowed
>>>>>>             parsed = parse_shim.urlparse(normalized_uri)
>>>>>>         except ValueError:
                   # URI is impossible to parse, therefore it's not allowed
>>>>>>             return None
       
>>>>>>         if parsed.scheme:
                   # If urlparse found a scheme, check that
>>>>>>             if parsed.scheme in allowed_protocols:
>>>>>>                 return value
       
               else:
                   # Allow uris that are just an anchor
>>>>>>             if normalized_uri.startswith("#"):
>>>>>>                 return value
       
                   # Handle protocols that urlparse doesn't recognize like "myprotocol"
>>>>>>             if (
>>>>>>                 ":" in normalized_uri
>>>>>>                 and normalized_uri.split(":")[0] in allowed_protocols
                   ):
>>>>>>                 return value
       
                   # If there's no protocol/scheme specified, then assume it's "http" or
                   # "https" and see if that's allowed
>>>>>>             if "http" in allowed_protocols or "https" in allowed_protocols:
>>>>>>                 return value
       
>>>>>>         return None
       
    1:     def allow_token(self, token):
               """Handles the case where we're allowing the tag"""
>>>>>>         if "data" in token:
                   # Loop through all the attributes and drop the ones that are not
                   # allowed, are unsafe or break other rules. Additionally, fix
                   # attribute values that need fixing.
                   #
                   # At the end of this loop, we have the final set of attributes
                   # we're keeping.
>>>>>>             attrs = {}
>>>>>>             for namespaced_name, val in token["data"].items():
>>>>>>                 namespace, name = namespaced_name
       
                       # Drop attributes that are not explicitly allowed
                       #
                       # NOTE(willkg): We pass in the attribute name--not a namespaced
                       # name.
>>>>>>                 if not self.attr_filter(token["name"], name, val):
>>>>>>                     continue
       
                       # Drop attributes with uri values that use a disallowed protocol
                       # Sanitize attributes with uri values
>>>>>>                 if namespaced_name in self.attr_val_is_uri:
>>>>>>                     new_value = self.sanitize_uri_value(val, self.allowed_protocols)
>>>>>>                     if new_value is None:
>>>>>>                         continue
>>>>>>                     val = new_value
       
                       # Drop values in svg attrs with non-local IRIs
>>>>>>                 if namespaced_name in self.svg_attr_val_allows_ref:
>>>>>>                     new_val = re.sub(r"url\s*\(\s*[^#\s][^)]+?\)", " ", unescape(val))
>>>>>>                     new_val = new_val.strip()
>>>>>>                     if not new_val:
>>>>>>                         continue
       
                           else:
                               # Replace the val with the unescaped version because
                               # it's a iri
>>>>>>                         val = new_val
       
                       # Drop href and xlink:href attr for svg elements with non-local IRIs
>>>>>>                 if (None, token["name"]) in self.svg_allow_local_href:
>>>>>>                     if namespaced_name in [
>>>>>>                         (None, "href"),
>>>>>>                         (html5lib_shim.namespaces["xlink"], "href"),
                           ]:
>>>>>>                         if re.search(r"^\s*[^#\s]", val):
>>>>>>                             continue
       
                       # If it's a style attribute, sanitize it
>>>>>>                 if namespaced_name == (None, "style"):
>>>>>>                     if self.css_sanitizer:
>>>>>>                         val = self.css_sanitizer.sanitize_css(val)
                           else:
                               # FIXME(willkg): if style is allowed, but no
                               # css_sanitizer was set up, then this is probably a
                               # mistake and we should raise an error here
                               #
                               # For now, we're going to set the value to "" because
                               # there was no sanitizer set
>>>>>>                         val = ""
       
                       # At this point, we want to keep the attribute, so add it in
>>>>>>                 attrs[namespaced_name] = val
       
>>>>>>             token["data"] = attrs
       
>>>>>>         return token
       
    1:     def disallowed_token(self, token):
>>>>>>         token_type = token["type"]
>>>>>>         if token_type == "EndTag":
>>>>>>             token["data"] = f"</{token['name']}>"
       
>>>>>>         elif token["data"]:
>>>>>>             assert token_type in ("StartTag", "EmptyTag")
>>>>>>             attrs = []
>>>>>>             for (ns, name), v in token["data"].items():
                       # If we end up with a namespace, but no name, switch them so we
                       # have a valid name to use.
>>>>>>                 if ns and not name:
>>>>>>                     ns, name = name, ns
       
                       # Figure out namespaced name if the namespace is appropriate
                       # and exists; if the ns isn't in prefixes, then drop it.
>>>>>>                 if ns is None or ns not in html5lib_shim.prefixes:
>>>>>>                     namespaced_name = name
                       else:
>>>>>>                     namespaced_name = f"{html5lib_shim.prefixes[ns]}:{name}"
       
                       # NOTE(willkg): HTMLSerializer escapes attribute values
                       # already, so if we do it here (like HTMLSerializer does),
                       # then we end up double-escaping.
>>>>>>                 attrs.append(f' {namespaced_name}="{v}"')
>>>>>>             token["data"] = f"<{token['name']}{''.join(attrs)}>"
       
               else:
>>>>>>             token["data"] = f"<{token['name']}>"
       
>>>>>>         if token.get("selfClosing"):
>>>>>>             token["data"] = f"{token['data'][:-1]}/>"
       
>>>>>>         token["type"] = "Characters"
       
>>>>>>         del token["name"]
>>>>>>         return token
