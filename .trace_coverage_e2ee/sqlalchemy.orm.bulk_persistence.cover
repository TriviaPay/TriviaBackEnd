       # orm/bulk_persistence.py
       # Copyright (C) 2005-2024 the SQLAlchemy authors and contributors
       # <see AUTHORS file>
       #
       # This module is part of SQLAlchemy and is released under
       # the MIT License: https://www.opensource.org/licenses/mit-license.php
       # mypy: ignore-errors
       
       
    1: """additional ORM persistence classes related to "bulk" operations,
       specifically outside of the flush() process.
       
       """
       
    1: from __future__ import annotations
       
    1: from typing import Any
    1: from typing import cast
    1: from typing import Dict
    1: from typing import Iterable
    1: from typing import Optional
    1: from typing import overload
    1: from typing import TYPE_CHECKING
    1: from typing import TypeVar
    1: from typing import Union
       
    1: from . import attributes
    1: from . import context
    1: from . import evaluator
    1: from . import exc as orm_exc
    1: from . import loading
    1: from . import persistence
    1: from .base import NO_VALUE
    1: from .context import AbstractORMCompileState
    1: from .context import FromStatement
    1: from .context import ORMFromStatementCompileState
    1: from .context import QueryContext
    1: from .. import exc as sa_exc
    1: from .. import util
    1: from ..engine import Dialect
    1: from ..engine import result as _result
    1: from ..sql import coercions
    1: from ..sql import dml
    1: from ..sql import expression
    1: from ..sql import roles
    1: from ..sql import select
    1: from ..sql import sqltypes
    1: from ..sql.base import _entity_namespace_key
    1: from ..sql.base import CompileState
    1: from ..sql.base import Options
    1: from ..sql.dml import DeleteDMLState
    1: from ..sql.dml import InsertDMLState
    1: from ..sql.dml import UpdateDMLState
    1: from ..util import EMPTY_DICT
    1: from ..util.typing import Literal
       
    1: if TYPE_CHECKING:
>>>>>>     from ._typing import DMLStrategyArgument
>>>>>>     from ._typing import OrmExecuteOptionsParameter
>>>>>>     from ._typing import SynchronizeSessionArgument
>>>>>>     from .mapper import Mapper
>>>>>>     from .session import _BindArguments
>>>>>>     from .session import ORMExecuteState
>>>>>>     from .session import Session
>>>>>>     from .session import SessionTransaction
>>>>>>     from .state import InstanceState
>>>>>>     from ..engine import Connection
>>>>>>     from ..engine import cursor
>>>>>>     from ..engine.interfaces import _CoreAnyExecuteParams
       
    1: _O = TypeVar("_O", bound=object)
       
       
    1: @overload
    1: def _bulk_insert(
           mapper: Mapper[_O],
           mappings: Union[Iterable[InstanceState[_O]], Iterable[Dict[str, Any]]],
           session_transaction: SessionTransaction,
           isstates: bool,
           return_defaults: bool,
           render_nulls: bool,
           use_orm_insert_stmt: Literal[None] = ...,
           execution_options: Optional[OrmExecuteOptionsParameter] = ...,
>>>>>> ) -> None: ...
       
       
    1: @overload
    1: def _bulk_insert(
           mapper: Mapper[_O],
           mappings: Union[Iterable[InstanceState[_O]], Iterable[Dict[str, Any]]],
           session_transaction: SessionTransaction,
           isstates: bool,
           return_defaults: bool,
           render_nulls: bool,
           use_orm_insert_stmt: Optional[dml.Insert] = ...,
           execution_options: Optional[OrmExecuteOptionsParameter] = ...,
>>>>>> ) -> cursor.CursorResult[Any]: ...
       
       
    1: def _bulk_insert(
           mapper: Mapper[_O],
           mappings: Union[Iterable[InstanceState[_O]], Iterable[Dict[str, Any]]],
           session_transaction: SessionTransaction,
           isstates: bool,
           return_defaults: bool,
           render_nulls: bool,
           use_orm_insert_stmt: Optional[dml.Insert] = None,
           execution_options: Optional[OrmExecuteOptionsParameter] = None,
       ) -> Optional[cursor.CursorResult[Any]]:
    2:     base_mapper = mapper.base_mapper
       
    2:     if session_transaction.session.connection_callable:
>>>>>>         raise NotImplementedError(
>>>>>>             "connection_callable / per-instance sharding "
                   "not supported in bulk_insert()"
               )
       
    2:     if isstates:
    2:         if return_defaults:
>>>>>>             states = [(state, state.dict) for state in mappings]
>>>>>>             mappings = [dict_ for (state, dict_) in states]
               else:
    7:             mappings = [state.dict for state in mappings]
           else:
>>>>>>         mappings = [dict(m) for m in mappings]
>>>>>>         _expand_composites(mapper, mappings)
       
    2:     connection = session_transaction.connection(base_mapper)
       
    2:     return_result: Optional[cursor.CursorResult[Any]] = None
       
    8:     mappers_to_run = [
    2:         (table, mp)
    4:         for table, mp in base_mapper._sorted_tables.items()
    2:         if table in mapper._pks_by_table
           ]
       
    2:     if return_defaults:
               # not used by new-style bulk inserts, only used for legacy
>>>>>>         bookkeeping = True
    2:     elif len(mappers_to_run) > 1:
               # if we have more than one table, mapper to run where we will be
               # either horizontally splicing, or copying values between tables,
               # we need the "bookkeeping" / deterministic returning order
>>>>>>         bookkeeping = True
           else:
    2:         bookkeeping = False
       
    4:     for table, super_mapper in mappers_to_run:
               # find bindparams in the statement. For bulk, we don't really know if
               # a key in the params applies to a different table since we are
               # potentially inserting for multiple tables here; looking at the
               # bindparam() is a lot more direct.   in most cases this will
               # use _generate_cache_key() which is memoized, although in practice
               # the ultimate statement that's executed is probably not the same
               # object so that memoization might not matter much.
    2:         extra_bp_names = (
    2:             [
>>>>>>                 b.key
>>>>>>                 for b in use_orm_insert_stmt._get_embedded_bindparams()
>>>>>>                 if b.key in mappings[0]
                   ]
    2:             if use_orm_insert_stmt is not None
    2:             else ()
               )
       
    9:         records = (
    3:             (
    3:                 None,
    3:                 state_dict,
    3:                 params,
    3:                 mapper,
    3:                 connection,
    3:                 value_params,
    3:                 has_all_pks,
    3:                 has_all_defaults,
                   )
    3:             for (
    3:                 state,
    3:                 state_dict,
    3:                 params,
    3:                 mp,
    3:                 conn,
    3:                 value_params,
    3:                 has_all_pks,
    3:                 has_all_defaults,
    4:             ) in persistence._collect_insert_commands(
    2:                 table,
    7:                 ((None, mapping, mapper, connection) for mapping in mappings),
    2:                 bulk=True,
    2:                 return_defaults=bookkeeping,
    2:                 render_nulls=render_nulls,
    2:                 include_bulk_keys=extra_bp_names,
                   )
               )
       
    4:         result = persistence._emit_insert_statements(
    2:             base_mapper,
    2:             None,
    2:             super_mapper,
    2:             table,
    2:             records,
    2:             bookkeeping=bookkeeping,
    2:             use_orm_insert_stmt=use_orm_insert_stmt,
    2:             execution_options=execution_options,
               )
    2:         if use_orm_insert_stmt is not None:
>>>>>>             if not use_orm_insert_stmt._returning or return_result is None:
>>>>>>                 return_result = result
>>>>>>             elif result.returns_rows:
>>>>>>                 assert bookkeeping
>>>>>>                 return_result = return_result.splice_horizontally(result)
       
    2:     if return_defaults and isstates:
>>>>>>         identity_cls = mapper._identity_class
>>>>>>         identity_props = [p.key for p in mapper._identity_key_props]
>>>>>>         for state, dict_ in states:
>>>>>>             state.key = (
>>>>>>                 identity_cls,
>>>>>>                 tuple([dict_[key] for key in identity_props]),
                   )
       
    2:     if use_orm_insert_stmt is not None:
>>>>>>         assert return_result is not None
>>>>>>         return return_result
       
       
    1: @overload
    1: def _bulk_update(
           mapper: Mapper[Any],
           mappings: Union[Iterable[InstanceState[_O]], Iterable[Dict[str, Any]]],
           session_transaction: SessionTransaction,
           isstates: bool,
           update_changed_only: bool,
           use_orm_update_stmt: Literal[None] = ...,
           enable_check_rowcount: bool = True,
>>>>>> ) -> None: ...
       
       
    1: @overload
    1: def _bulk_update(
           mapper: Mapper[Any],
           mappings: Union[Iterable[InstanceState[_O]], Iterable[Dict[str, Any]]],
           session_transaction: SessionTransaction,
           isstates: bool,
           update_changed_only: bool,
           use_orm_update_stmt: Optional[dml.Update] = ...,
           enable_check_rowcount: bool = True,
>>>>>> ) -> _result.Result[Any]: ...
       
       
    1: def _bulk_update(
           mapper: Mapper[Any],
           mappings: Union[Iterable[InstanceState[_O]], Iterable[Dict[str, Any]]],
           session_transaction: SessionTransaction,
           isstates: bool,
           update_changed_only: bool,
           use_orm_update_stmt: Optional[dml.Update] = None,
           enable_check_rowcount: bool = True,
       ) -> Optional[_result.Result[Any]]:
>>>>>>     base_mapper = mapper.base_mapper
       
>>>>>>     search_keys = mapper._primary_key_propkeys
>>>>>>     if mapper._version_id_prop:
>>>>>>         search_keys = {mapper._version_id_prop.key}.union(search_keys)
       
>>>>>>     def _changed_dict(mapper, state):
>>>>>>         return {
>>>>>>             k: v
>>>>>>             for k, v in state.dict.items()
>>>>>>             if k in state.committed_state or k in search_keys
               }
       
>>>>>>     if isstates:
>>>>>>         if update_changed_only:
>>>>>>             mappings = [_changed_dict(mapper, state) for state in mappings]
               else:
>>>>>>             mappings = [state.dict for state in mappings]
           else:
>>>>>>         mappings = [dict(m) for m in mappings]
>>>>>>         _expand_composites(mapper, mappings)
       
>>>>>>     if session_transaction.session.connection_callable:
>>>>>>         raise NotImplementedError(
>>>>>>             "connection_callable / per-instance sharding "
                   "not supported in bulk_update()"
               )
       
>>>>>>     connection = session_transaction.connection(base_mapper)
       
           # find bindparams in the statement. see _bulk_insert for similar
           # notes for the insert case
>>>>>>     extra_bp_names = (
>>>>>>         [
>>>>>>             b.key
>>>>>>             for b in use_orm_update_stmt._get_embedded_bindparams()
>>>>>>             if b.key in mappings[0]
               ]
>>>>>>         if use_orm_update_stmt is not None
>>>>>>         else ()
           )
       
>>>>>>     for table, super_mapper in base_mapper._sorted_tables.items():
>>>>>>         if not mapper.isa(super_mapper) or table not in mapper._pks_by_table:
>>>>>>             continue
       
>>>>>>         records = persistence._collect_update_commands(
>>>>>>             None,
>>>>>>             table,
>>>>>>             (
>>>>>>                 (
>>>>>>                     None,
>>>>>>                     mapping,
>>>>>>                     mapper,
>>>>>>                     connection,
                           (
>>>>>>                         mapping[mapper._version_id_prop.key]
>>>>>>                         if mapper._version_id_prop
>>>>>>                         else None
                           ),
                       )
>>>>>>                 for mapping in mappings
                   ),
>>>>>>             bulk=True,
>>>>>>             use_orm_update_stmt=use_orm_update_stmt,
>>>>>>             include_bulk_keys=extra_bp_names,
               )
>>>>>>         persistence._emit_update_statements(
>>>>>>             base_mapper,
>>>>>>             None,
>>>>>>             super_mapper,
>>>>>>             table,
>>>>>>             records,
>>>>>>             bookkeeping=False,
>>>>>>             use_orm_update_stmt=use_orm_update_stmt,
>>>>>>             enable_check_rowcount=enable_check_rowcount,
               )
       
>>>>>>     if use_orm_update_stmt is not None:
>>>>>>         return _result.null_result()
       
       
    1: def _expand_composites(mapper, mappings):
>>>>>>     composite_attrs = mapper.composites
>>>>>>     if not composite_attrs:
>>>>>>         return
       
>>>>>>     composite_keys = set(composite_attrs.keys())
>>>>>>     populators = {
>>>>>>         key: composite_attrs[key]._populate_composite_bulk_save_mappings_fn()
>>>>>>         for key in composite_keys
           }
>>>>>>     for mapping in mappings:
>>>>>>         for key in composite_keys.intersection(mapping):
>>>>>>             populators[key](mapping)
       
       
    2: class ORMDMLState(AbstractORMCompileState):
    1:     is_dml_returning = True
    1:     from_statement_ctx: Optional[ORMFromStatementCompileState] = None
       
    1:     @classmethod
    1:     def _get_orm_crud_kv_pairs(
               cls, mapper, statement, kv_iterator, needs_to_be_cacheable
           ):
    3:         core_get_crud_kv_pairs = UpdateDMLState._get_crud_kv_pairs
       
    6:         for k, v in kv_iterator:
    3:             k = coercions.expect(roles.DMLColumnRole, k)
       
    3:             if isinstance(k, str):
    3:                 desc = _entity_namespace_key(mapper, k, default=NO_VALUE)
    3:                 if desc is NO_VALUE:
>>>>>>                     yield (
>>>>>>                         coercions.expect(roles.DMLColumnRole, k),
                               (
>>>>>>                             coercions.expect(
>>>>>>                                 roles.ExpressionElementRole,
>>>>>>                                 v,
>>>>>>                                 type_=sqltypes.NullType(),
>>>>>>                                 is_crud=True,
                                   )
>>>>>>                             if needs_to_be_cacheable
>>>>>>                             else v
                               ),
                           )
                       else:
    6:                     yield from core_get_crud_kv_pairs(
    3:                         statement,
    3:                         desc._bulk_update_tuples(v),
    3:                         needs_to_be_cacheable,
                           )
>>>>>>             elif "entity_namespace" in k._annotations:
>>>>>>                 k_anno = k._annotations
>>>>>>                 attr = _entity_namespace_key(
>>>>>>                     k_anno["entity_namespace"], k_anno["proxy_key"]
                       )
>>>>>>                 yield from core_get_crud_kv_pairs(
>>>>>>                     statement,
>>>>>>                     attr._bulk_update_tuples(v),
>>>>>>                     needs_to_be_cacheable,
                       )
                   else:
>>>>>>                 yield (
>>>>>>                     k,
                           (
>>>>>>                         v
>>>>>>                         if not needs_to_be_cacheable
>>>>>>                         else coercions.expect(
>>>>>>                             roles.ExpressionElementRole,
>>>>>>                             v,
>>>>>>                             type_=sqltypes.NullType(),
>>>>>>                             is_crud=True,
                               )
                           ),
                       )
       
    1:     @classmethod
    1:     def _get_multi_crud_kv_pairs(cls, statement, kv_iterator):
>>>>>>         plugin_subject = statement._propagate_attrs["plugin_subject"]
       
>>>>>>         if not plugin_subject or not plugin_subject.mapper:
>>>>>>             return UpdateDMLState._get_multi_crud_kv_pairs(
>>>>>>                 statement, kv_iterator
                   )
       
>>>>>>         return [
>>>>>>             dict(
>>>>>>                 cls._get_orm_crud_kv_pairs(
>>>>>>                     plugin_subject.mapper, statement, value_dict.items(), False
                       )
                   )
>>>>>>             for value_dict in kv_iterator
               ]
       
    1:     @classmethod
    1:     def _get_crud_kv_pairs(cls, statement, kv_iterator, needs_to_be_cacheable):
    3:         assert (
    3:             needs_to_be_cacheable
>>>>>>         ), "no test coverage for needs_to_be_cacheable=False"
       
    3:         plugin_subject = statement._propagate_attrs["plugin_subject"]
       
    3:         if not plugin_subject or not plugin_subject.mapper:
>>>>>>             return UpdateDMLState._get_crud_kv_pairs(
>>>>>>                 statement, kv_iterator, needs_to_be_cacheable
                   )
       
    6:         return list(
    6:             cls._get_orm_crud_kv_pairs(
    3:                 plugin_subject.mapper,
    3:                 statement,
    3:                 kv_iterator,
    3:                 needs_to_be_cacheable,
                   )
               )
       
    1:     @classmethod
    1:     def get_entity_description(cls, statement):
>>>>>>         ext_info = statement.table._annotations["parententity"]
>>>>>>         mapper = ext_info.mapper
>>>>>>         if ext_info.is_aliased_class:
>>>>>>             _label_name = ext_info.name
               else:
>>>>>>             _label_name = mapper.class_.__name__
       
>>>>>>         return {
>>>>>>             "name": _label_name,
>>>>>>             "type": mapper.class_,
>>>>>>             "expr": ext_info.entity,
>>>>>>             "entity": ext_info.entity,
>>>>>>             "table": mapper.local_table,
               }
       
    1:     @classmethod
    1:     def get_returning_column_descriptions(cls, statement):
>>>>>>         def _ent_for_col(c):
>>>>>>             return c._annotations.get("parententity", None)
       
>>>>>>         def _attr_for_col(c, ent):
>>>>>>             if ent is None:
>>>>>>                 return c
>>>>>>             proxy_key = c._annotations.get("proxy_key", None)
>>>>>>             if not proxy_key:
>>>>>>                 return c
                   else:
>>>>>>                 return getattr(ent.entity, proxy_key, c)
       
>>>>>>         return [
>>>>>>             {
>>>>>>                 "name": c.key,
>>>>>>                 "type": c.type,
>>>>>>                 "expr": _attr_for_col(c, ent),
>>>>>>                 "aliased": ent.is_aliased_class,
>>>>>>                 "entity": ent.entity,
                   }
>>>>>>             for c, ent in [
>>>>>>                 (c, _ent_for_col(c)) for c in statement._all_selected_columns
                   ]
               ]
       
    1:     def _setup_orm_returning(
               self,
               compiler,
               orm_level_statement,
               dml_level_statement,
               dml_mapper,
               *,
    1:         use_supplemental_cols=True,
           ):
               """establish ORM column handlers for an INSERT, UPDATE, or DELETE
               which uses explicit returning().
       
               called within compilation level create_for_statement.
       
               The _return_orm_returning() method then receives the Result
               after the statement was executed, and applies ORM loading to the
               state that we first established here.
       
               """
       
    2:         if orm_level_statement._returning:
    2:             fs = FromStatement(
    1:                 orm_level_statement._returning,
    1:                 dml_level_statement,
    1:                 _adapt_on_names=False,
                   )
    1:             fs = fs.execution_options(**orm_level_statement._execution_options)
    1:             fs = fs.options(*orm_level_statement._with_options)
    1:             self.select_statement = fs
    1:             self.from_statement_ctx = fsc = (
    1:                 ORMFromStatementCompileState.create_for_statement(fs, compiler)
                   )
    1:             fsc.setup_dml_returning_compile_state(dml_mapper)
       
    1:             dml_level_statement = dml_level_statement._generate()
    1:             dml_level_statement._returning = ()
       
    3:             cols_to_return = [c for c in fsc.primary_columns if c is not None]
       
                   # since we are splicing result sets together, make sure there
                   # are columns of some kind returned in each result set
    1:             if not cols_to_return:
>>>>>>                 cols_to_return.extend(dml_mapper.primary_key)
       
    1:             if use_supplemental_cols:
>>>>>>                 dml_level_statement = dml_level_statement.return_defaults(
                           # this is a little weird looking, but by passing
                           # primary key as the main list of cols, this tells
                           # return_defaults to omit server-default cols (and
                           # actually all cols, due to some weird thing we should
                           # clean up in crud.py).
                           # Since we have cols_to_return, just return what we asked
                           # for (plus primary key, which ORM persistence needs since
                           # we likely set bookkeeping=True here, which is another
                           # whole thing...).   We dont want to clutter the
                           # statement up with lots of other cols the user didn't
                           # ask for.  see #9685
>>>>>>                     *dml_mapper.primary_key,
>>>>>>                     supplemental_cols=cols_to_return,
                       )
                   else:
    2:                 dml_level_statement = dml_level_statement.returning(
    1:                     *cols_to_return
                       )
       
    2:         return dml_level_statement
       
    1:     @classmethod
    1:     def _return_orm_returning(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               result,
           ):
    5:         execution_context = result.context
    5:         compile_state = execution_context.compiled.compile_state
       
    8:         if (
    5:             compile_state.from_statement_ctx
    3:             and not compile_state.from_statement_ctx.compile_options._is_star
               ):
    6:             load_options = execution_options.get(
    3:                 "_sa_orm_load_options", QueryContext.default_load_options
                   )
       
    6:             querycontext = QueryContext(
    3:                 compile_state.from_statement_ctx,
    3:                 compile_state.select_statement,
    3:                 params,
    3:                 session,
    3:                 load_options,
    3:                 execution_options,
    3:                 bind_arguments,
                   )
    3:             return loading.instances(result, querycontext)
               else:
    2:             return result
       
       
    2: class BulkUDCompileState(ORMDMLState):
    2:     class default_update_options(Options):
    1:         _dml_strategy: DMLStrategyArgument = "auto"
    1:         _synchronize_session: SynchronizeSessionArgument = "auto"
    1:         _can_use_returning: bool = False
    1:         _is_delete_using: bool = False
    1:         _is_update_from: bool = False
    1:         _autoflush: bool = True
    1:         _subject_mapper: Optional[Mapper[Any]] = None
    1:         _resolved_values = EMPTY_DICT
    1:         _eval_condition = None
    1:         _matched_rows = None
    1:         _identity_token = None
       
    1:     @classmethod
    1:     def can_use_returning(
               cls,
               dialect: Dialect,
               mapper: Mapper[Any],
               *,
    1:         is_multitable: bool = False,
    1:         is_update_from: bool = False,
    1:         is_delete_using: bool = False,
    1:         is_executemany: bool = False,
           ) -> bool:
>>>>>>         raise NotImplementedError()
       
    1:     @classmethod
    1:     def orm_pre_session_exec(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               is_pre_event,
           ):
    5:         (
    5:             update_options,
    5:             execution_options,
   10:         ) = BulkUDCompileState.default_update_options.from_execution_options(
    5:             "_sa_orm_update_options",
    5:             {
                       "synchronize_session",
                       "autoflush",
                       "identity_token",
                       "is_delete_using",
                       "is_update_from",
                       "dml_strategy",
                   },
    5:             execution_options,
    5:             statement._execution_options,
               )
    5:         bind_arguments["clause"] = statement
    5:         try:
    5:             plugin_subject = statement._propagate_attrs["plugin_subject"]
>>>>>>         except KeyError:
>>>>>>             assert False, "statement had 'orm' plugin but no plugin_subject"
               else:
    5:             if plugin_subject:
    5:                 bind_arguments["mapper"] = plugin_subject.mapper
    5:                 update_options += {"_subject_mapper": plugin_subject.mapper}
       
    5:         if "parententity" not in statement.table._annotations:
>>>>>>             update_options += {"_dml_strategy": "core_only"}
    5:         elif not isinstance(params, list):
    5:             if update_options._dml_strategy == "auto":
    5:                 update_options += {"_dml_strategy": "orm"}
>>>>>>             elif update_options._dml_strategy == "bulk":
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     'Can\'t use "bulk" ORM insert strategy without '
                           "passing separate parameters"
                       )
               else:
>>>>>>             if update_options._dml_strategy == "auto":
>>>>>>                 update_options += {"_dml_strategy": "bulk"}
       
    5:         sync = update_options._synchronize_session
    5:         if sync is not None:
    5:             if sync not in ("auto", "evaluate", "fetch", False):
>>>>>>                 raise sa_exc.ArgumentError(
>>>>>>                     "Valid strategies for session synchronization "
                           "are 'auto', 'evaluate', 'fetch', False"
                       )
    5:             if update_options._dml_strategy == "bulk" and sync == "fetch":
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     "The 'fetch' synchronization strategy is not available "
                           "for 'bulk' ORM updates (i.e. multiple parameter sets)"
                       )
       
    5:         if not is_pre_event:
    5:             if update_options._autoflush:
    5:                 session._autoflush()
       
    5:             if update_options._dml_strategy == "orm":
    5:                 if update_options._synchronize_session == "auto":
    6:                     update_options = cls._do_pre_synchronize_auto(
    3:                         session,
    3:                         statement,
    3:                         params,
    3:                         execution_options,
    3:                         bind_arguments,
    3:                         update_options,
                           )
    2:                 elif update_options._synchronize_session == "evaluate":
>>>>>>                     update_options = cls._do_pre_synchronize_evaluate(
>>>>>>                         session,
>>>>>>                         statement,
>>>>>>                         params,
>>>>>>                         execution_options,
>>>>>>                         bind_arguments,
>>>>>>                         update_options,
                           )
    2:                 elif update_options._synchronize_session == "fetch":
>>>>>>                     update_options = cls._do_pre_synchronize_fetch(
>>>>>>                         session,
>>>>>>                         statement,
>>>>>>                         params,
>>>>>>                         execution_options,
>>>>>>                         bind_arguments,
>>>>>>                         update_options,
                           )
>>>>>>             elif update_options._dml_strategy == "bulk":
>>>>>>                 if update_options._synchronize_session == "auto":
>>>>>>                     update_options += {"_synchronize_session": "evaluate"}
       
                   # indicators from the "pre exec" step that are then
                   # added to the DML statement, which will also be part of the cache
                   # key.  The compile level create_for_statement() method will then
                   # consume these at compiler time.
   10:             statement = statement._annotate(
    5:                 {
    5:                     "synchronize_session": update_options._synchronize_session,
    5:                     "is_delete_using": update_options._is_delete_using,
    5:                     "is_update_from": update_options._is_update_from,
    5:                     "dml_strategy": update_options._dml_strategy,
    5:                     "can_use_returning": update_options._can_use_returning,
                       }
                   )
       
    5:         return (
    5:             statement,
   10:             util.immutabledict(execution_options).union(
    5:                 {"_sa_orm_update_options": update_options}
                   ),
               )
       
    1:     @classmethod
    1:     def orm_setup_cursor_result(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               result,
           ):
               # this stage of the execution is called after the
               # do_orm_execute event hook.  meaning for an extension like
               # horizontal sharding, this step happens *within* the horizontal
               # sharding event handler which calls session.execute() re-entrantly
               # and will occur for each backend individually.
               # the sharding extension then returns its own merged result from the
               # individual ones we return here.
       
    5:         update_options = execution_options["_sa_orm_update_options"]
    5:         if update_options._dml_strategy == "orm":
    5:             if update_options._synchronize_session == "evaluate":
    6:                 cls._do_post_synchronize_evaluate(
    3:                     session, statement, result, update_options
                       )
    2:             elif update_options._synchronize_session == "fetch":
>>>>>>                 cls._do_post_synchronize_fetch(
>>>>>>                     session, statement, result, update_options
                       )
>>>>>>         elif update_options._dml_strategy == "bulk":
>>>>>>             if update_options._synchronize_session == "evaluate":
>>>>>>                 cls._do_post_synchronize_bulk_evaluate(
>>>>>>                     session, params, result, update_options
                       )
>>>>>>             return result
       
   10:         return cls._return_orm_returning(
    5:             session,
    5:             statement,
    5:             params,
    5:             execution_options,
    5:             bind_arguments,
    5:             result,
               )
       
    1:     @classmethod
    1:     def _adjust_for_extra_criteria(cls, global_attributes, ext_info):
               """Apply extra criteria filtering.
       
               For all distinct single-table-inheritance mappers represented in the
               table being updated or deleted, produce additional WHERE criteria such
               that only the appropriate subtypes are selected from the total results.
       
               Additionally, add WHERE criteria originating from LoaderCriteriaOptions
               collected from the statement.
       
               """
       
    2:         return_crit = ()
       
    2:         adapter = ext_info._adapter if ext_info.is_aliased_class else None
       
    4:         if (
    2:             "additional_entity_criteria",
    2:             ext_info.mapper,
    2:         ) in global_attributes:
>>>>>>             return_crit += tuple(
>>>>>>                 ae._resolve_where_criteria(ext_info)
>>>>>>                 for ae in global_attributes[
>>>>>>                     ("additional_entity_criteria", ext_info.mapper)
                       ]
>>>>>>                 if ae.include_aliases or ae.entity is ext_info
                   )
       
    2:         if ext_info.mapper._single_table_criterion is not None:
>>>>>>             return_crit += (ext_info.mapper._single_table_criterion,)
       
    2:         if adapter:
>>>>>>             return_crit = tuple(adapter.traverse(crit) for crit in return_crit)
       
    2:         return return_crit
       
    1:     @classmethod
    1:     def _interpret_returning_rows(cls, mapper, rows):
               """translate from local inherited table columns to base mapper
               primary key columns.
       
               Joined inheritance mappers always establish the primary key in terms of
               the base table.   When we UPDATE a sub-table, we can only get
               RETURNING for the sub-table's columns.
       
               Here, we create a lookup from the local sub table's primary key
               columns to the base table PK columns so that we can get identity
               key values from RETURNING that's against the joined inheritance
               sub-table.
       
               the complexity here is to support more than one level deep of
               inheritance, where we have to link columns to each other across
               the inheritance hierarchy.
       
               """
       
>>>>>>         if mapper.local_table is not mapper.base_mapper.local_table:
>>>>>>             return rows
       
               # this starts as a mapping of
               # local_pk_col: local_pk_col.
               # we will then iteratively rewrite the "value" of the dict with
               # each successive superclass column
>>>>>>         local_pk_to_base_pk = {pk: pk for pk in mapper.local_table.primary_key}
       
>>>>>>         for mp in mapper.iterate_to_root():
>>>>>>             if mp.inherits is None:
>>>>>>                 break
>>>>>>             elif mp.local_table is mp.inherits.local_table:
>>>>>>                 continue
       
>>>>>>             t_to_e = dict(mp._table_to_equated[mp.inherits.local_table])
>>>>>>             col_to_col = {sub_pk: super_pk for super_pk, sub_pk in t_to_e[mp]}
>>>>>>             for pk, super_ in local_pk_to_base_pk.items():
>>>>>>                 local_pk_to_base_pk[pk] = col_to_col[super_]
       
>>>>>>         lookup = {
>>>>>>             local_pk_to_base_pk[lpk]: idx
>>>>>>             for idx, lpk in enumerate(mapper.local_table.primary_key)
               }
>>>>>>         primary_key_convert = [
>>>>>>             lookup[bpk] for bpk in mapper.base_mapper.primary_key
               ]
>>>>>>         return [tuple(row[idx] for idx in primary_key_convert) for row in rows]
       
    1:     @classmethod
    1:     def _get_matched_objects_on_criteria(cls, update_options, states):
    3:         mapper = update_options._subject_mapper
    3:         eval_condition = update_options._eval_condition
       
   20:         raw_data = [
    2:             (state.obj(), state, state.dict)
   14:             for state in states
   11:             if state.mapper.isa(mapper) and not state.expired
               ]
       
    3:         identity_token = update_options._identity_token
    3:         if identity_token is not None:
>>>>>>             raw_data = [
>>>>>>                 (obj, state, dict_)
>>>>>>                 for obj, state, dict_ in raw_data
>>>>>>                 if state.identity_token == identity_token
                   ]
       
    3:         result = []
    5:         for obj, state, dict_ in raw_data:
    2:             evaled_condition = eval_condition(obj)
       
                   # caution: don't use "in ()" or == here, _EXPIRE_OBJECT
                   # evaluates as True for all comparisons
    3:             if (
    2:                 evaled_condition is True
    1:                 or evaled_condition is evaluator._EXPIRED_OBJECT
                   ):
    2:                 result.append(
    1:                     (
    1:                         obj,
    1:                         state,
    1:                         dict_,
    1:                         evaled_condition is evaluator._EXPIRED_OBJECT,
                           )
                       )
    3:         return result
       
    1:     @classmethod
    1:     def _eval_condition_from_statement(cls, update_options, statement):
    3:         mapper = update_options._subject_mapper
    3:         target_cls = mapper.class_
       
    3:         evaluator_compiler = evaluator._EvaluatorCompiler(target_cls)
    3:         crit = ()
    3:         if statement._where_criteria:
    3:             crit += statement._where_criteria
       
    3:         global_attributes = {}
    3:         for opt in statement._with_options:
>>>>>>             if opt._is_criteria_option:
>>>>>>                 opt.get_global_criteria(global_attributes)
       
    3:         if global_attributes:
>>>>>>             crit += cls._adjust_for_extra_criteria(global_attributes, mapper)
       
    3:         if crit:
    3:             eval_condition = evaluator_compiler.process(*crit)
               else:
                   # workaround for mypy https://github.com/python/mypy/issues/14027
>>>>>>             def _eval_condition(obj):
>>>>>>                 return True
       
>>>>>>             eval_condition = _eval_condition
       
    3:         return eval_condition
       
    1:     @classmethod
    1:     def _do_pre_synchronize_auto(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               update_options,
           ):
               """setup auto sync strategy
       
       
               "auto" checks if we can use "evaluate" first, then falls back
               to "fetch"
       
               evaluate is vastly more efficient for the common case
               where session is empty, only has a few objects, and the UPDATE
               statement can potentially match thousands/millions of rows.
       
               OTOH more complex criteria that fails to work with "evaluate"
               we would hope usually correlates with fewer net rows.
       
               """
       
    3:         try:
    6:             eval_condition = cls._eval_condition_from_statement(
    3:                 update_options, statement
                   )
       
>>>>>>         except evaluator.UnevaluatableError:
>>>>>>             pass
               else:
    6:             return update_options + {
    3:                 "_eval_condition": eval_condition,
    3:                 "_synchronize_session": "evaluate",
                   }
       
>>>>>>         update_options += {"_synchronize_session": "fetch"}
>>>>>>         return cls._do_pre_synchronize_fetch(
>>>>>>             session,
>>>>>>             statement,
>>>>>>             params,
>>>>>>             execution_options,
>>>>>>             bind_arguments,
>>>>>>             update_options,
               )
       
    1:     @classmethod
    1:     def _do_pre_synchronize_evaluate(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               update_options,
           ):
>>>>>>         try:
>>>>>>             eval_condition = cls._eval_condition_from_statement(
>>>>>>                 update_options, statement
                   )
       
>>>>>>         except evaluator.UnevaluatableError as err:
>>>>>>             raise sa_exc.InvalidRequestError(
>>>>>>                 'Could not evaluate current criteria in Python: "%s". '
                       "Specify 'fetch' or False for the "
>>>>>>                 "synchronize_session execution option." % err
>>>>>>             ) from err
       
>>>>>>         return update_options + {
>>>>>>             "_eval_condition": eval_condition,
               }
       
    1:     @classmethod
    1:     def _get_resolved_values(cls, mapper, statement):
    4:         if statement._multi_values:
>>>>>>             return []
    4:         elif statement._ordered_values:
>>>>>>             return list(statement._ordered_values)
    4:         elif statement._values:
    4:             return list(statement._values.items())
               else:
>>>>>>             return []
       
    1:     @classmethod
    1:     def _resolved_keys_as_propnames(cls, mapper, resolved_values):
    3:         values = []
    6:         for k, v in resolved_values:
    3:             if mapper and isinstance(k, expression.ColumnElement):
    3:                 try:
    3:                     attr = mapper._columntoproperty[k]
>>>>>>                 except orm_exc.UnmappedColumnError:
>>>>>>                     pass
                       else:
    3:                     values.append((attr.key, v))
                   else:
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     "Attribute name not found, can't be "
>>>>>>                     "synchronized back to objects: %r" % k
                       )
    3:         return values
       
    1:     @classmethod
    1:     def _do_pre_synchronize_fetch(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               update_options,
           ):
>>>>>>         mapper = update_options._subject_mapper
       
>>>>>>         select_stmt = (
>>>>>>             select(*(mapper.primary_key + (mapper.select_identity_token,)))
>>>>>>             .select_from(mapper)
>>>>>>             .options(*statement._with_options)
               )
>>>>>>         select_stmt._where_criteria = statement._where_criteria
       
               # conditionally run the SELECT statement for pre-fetch, testing the
               # "bind" for if we can use RETURNING or not using the do_orm_execute
               # event.  If RETURNING is available, the do_orm_execute event
               # will cancel the SELECT from being actually run.
               #
               # The way this is organized seems strange, why don't we just
               # call can_use_returning() before invoking the statement and get
               # answer?, why does this go through the whole execute phase using an
               # event?  Answer: because we are integrating with extensions such
               # as the horizontal sharding extention that "multiplexes" an individual
               # statement run through multiple engines, and it uses
               # do_orm_execute() to do that.
       
>>>>>>         can_use_returning = None
       
>>>>>>         def skip_for_returning(orm_context: ORMExecuteState) -> Any:
>>>>>>             bind = orm_context.session.get_bind(**orm_context.bind_arguments)
                   nonlocal can_use_returning
       
>>>>>>             per_bind_result = cls.can_use_returning(
>>>>>>                 bind.dialect,
>>>>>>                 mapper,
>>>>>>                 is_update_from=update_options._is_update_from,
>>>>>>                 is_delete_using=update_options._is_delete_using,
>>>>>>                 is_executemany=orm_context.is_executemany,
                   )
       
>>>>>>             if can_use_returning is not None:
>>>>>>                 if can_use_returning != per_bind_result:
>>>>>>                     raise sa_exc.InvalidRequestError(
>>>>>>                         "For synchronize_session='fetch', can't mix multiple "
                               "backends where some support RETURNING and others "
                               "don't"
                           )
>>>>>>             elif orm_context.is_executemany and not per_bind_result:
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     "For synchronize_session='fetch', can't use multiple "
                           "parameter sets in ORM mode, which this backend does not "
                           "support with RETURNING"
                       )
                   else:
>>>>>>                 can_use_returning = per_bind_result
       
>>>>>>             if per_bind_result:
>>>>>>                 return _result.null_result()
                   else:
>>>>>>                 return None
       
>>>>>>         result = session.execute(
>>>>>>             select_stmt,
>>>>>>             params,
>>>>>>             execution_options=execution_options,
>>>>>>             bind_arguments=bind_arguments,
>>>>>>             _add_event=skip_for_returning,
               )
>>>>>>         matched_rows = result.fetchall()
       
>>>>>>         return update_options + {
>>>>>>             "_matched_rows": matched_rows,
>>>>>>             "_can_use_returning": can_use_returning,
               }
       
       
    2: @CompileState.plugin_for("orm", "insert")
    1: class BulkORMInsert(ORMDMLState, InsertDMLState):
    2:     class default_insert_options(Options):
    1:         _dml_strategy: DMLStrategyArgument = "auto"
    1:         _render_nulls: bool = False
    1:         _return_defaults: bool = False
    1:         _subject_mapper: Optional[Mapper[Any]] = None
    1:         _autoflush: bool = True
    1:         _populate_existing: bool = False
       
    1:     select_statement: Optional[FromStatement] = None
       
    1:     @classmethod
    1:     def orm_pre_session_exec(
               cls,
               session,
               statement,
               params,
               execution_options,
               bind_arguments,
               is_pre_event,
           ):
>>>>>>         (
>>>>>>             insert_options,
>>>>>>             execution_options,
>>>>>>         ) = BulkORMInsert.default_insert_options.from_execution_options(
>>>>>>             "_sa_orm_insert_options",
>>>>>>             {"dml_strategy", "autoflush", "populate_existing", "render_nulls"},
>>>>>>             execution_options,
>>>>>>             statement._execution_options,
               )
>>>>>>         bind_arguments["clause"] = statement
>>>>>>         try:
>>>>>>             plugin_subject = statement._propagate_attrs["plugin_subject"]
>>>>>>         except KeyError:
>>>>>>             assert False, "statement had 'orm' plugin but no plugin_subject"
               else:
>>>>>>             if plugin_subject:
>>>>>>                 bind_arguments["mapper"] = plugin_subject.mapper
>>>>>>                 insert_options += {"_subject_mapper": plugin_subject.mapper}
       
>>>>>>         if not params:
>>>>>>             if insert_options._dml_strategy == "auto":
>>>>>>                 insert_options += {"_dml_strategy": "orm"}
>>>>>>             elif insert_options._dml_strategy == "bulk":
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     'Can\'t use "bulk" ORM insert strategy without '
                           "passing separate parameters"
                       )
               else:
>>>>>>             if insert_options._dml_strategy == "auto":
>>>>>>                 insert_options += {"_dml_strategy": "bulk"}
       
>>>>>>         if insert_options._dml_strategy != "raw":
                   # for ORM object loading, like ORMContext, we have to disable
                   # result set adapt_to_context, because we will be generating a
                   # new statement with specific columns that's cached inside of
                   # an ORMFromStatementCompileState, which we will re-use for
                   # each result.
>>>>>>             if not execution_options:
>>>>>>                 execution_options = context._orm_load_exec_options
                   else:
>>>>>>                 execution_options = execution_options.union(
>>>>>>                     context._orm_load_exec_options
                       )
       
>>>>>>         if not is_pre_event and insert_options._autoflush:
>>>>>>             session._autoflush()
       
>>>>>>         statement = statement._annotate(
>>>>>>             {"dml_strategy": insert_options._dml_strategy}
               )
       
>>>>>>         return (
>>>>>>             statement,
>>>>>>             util.immutabledict(execution_options).union(
>>>>>>                 {"_sa_orm_insert_options": insert_options}
                   ),
               )
       
    1:     @classmethod
    1:     def orm_execute_statement(
               cls,
               session: Session,
               statement: dml.Insert,
               params: _CoreAnyExecuteParams,
               execution_options: OrmExecuteOptionsParameter,
               bind_arguments: _BindArguments,
               conn: Connection,
           ) -> _result.Result:
>>>>>>         insert_options = execution_options.get(
>>>>>>             "_sa_orm_insert_options", cls.default_insert_options
               )
       
>>>>>>         if insert_options._dml_strategy not in (
                   "raw",
                   "bulk",
                   "orm",
                   "auto",
               ):
>>>>>>             raise sa_exc.ArgumentError(
>>>>>>                 "Valid strategies for ORM insert strategy "
                       "are 'raw', 'orm', 'bulk', 'auto"
                   )
       
               result: _result.Result[Any]
       
>>>>>>         if insert_options._dml_strategy == "raw":
>>>>>>             result = conn.execute(
>>>>>>                 statement, params or {}, execution_options=execution_options
                   )
>>>>>>             return result
       
>>>>>>         if insert_options._dml_strategy == "bulk":
>>>>>>             mapper = insert_options._subject_mapper
       
>>>>>>             if (
>>>>>>                 statement._post_values_clause is not None
>>>>>>                 and mapper._multiple_persistence_tables
                   ):
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     "bulk INSERT with a 'post values' clause "
                           "(typically upsert) not supported for multi-table "
>>>>>>                     f"mapper {mapper}"
                       )
       
>>>>>>             assert mapper is not None
>>>>>>             assert session._transaction is not None
>>>>>>             result = _bulk_insert(
>>>>>>                 mapper,
>>>>>>                 cast(
>>>>>>                     "Iterable[Dict[str, Any]]",
>>>>>>                     [params] if isinstance(params, dict) else params,
                       ),
>>>>>>                 session._transaction,
>>>>>>                 isstates=False,
>>>>>>                 return_defaults=insert_options._return_defaults,
>>>>>>                 render_nulls=insert_options._render_nulls,
>>>>>>                 use_orm_insert_stmt=statement,
>>>>>>                 execution_options=execution_options,
                   )
>>>>>>         elif insert_options._dml_strategy == "orm":
>>>>>>             result = conn.execute(
>>>>>>                 statement, params or {}, execution_options=execution_options
                   )
               else:
>>>>>>             raise AssertionError()
       
>>>>>>         if not bool(statement._returning):
>>>>>>             return result
       
>>>>>>         if insert_options._populate_existing:
>>>>>>             load_options = execution_options.get(
>>>>>>                 "_sa_orm_load_options", QueryContext.default_load_options
                   )
>>>>>>             load_options += {"_populate_existing": True}
>>>>>>             execution_options = execution_options.union(
>>>>>>                 {"_sa_orm_load_options": load_options}
                   )
       
>>>>>>         return cls._return_orm_returning(
>>>>>>             session,
>>>>>>             statement,
>>>>>>             params,
>>>>>>             execution_options,
>>>>>>             bind_arguments,
>>>>>>             result,
               )
       
    1:     @classmethod
    1:     def create_for_statement(cls, statement, compiler, **kw) -> BulkORMInsert:
>>>>>>         self = cast(
>>>>>>             BulkORMInsert,
>>>>>>             super().create_for_statement(statement, compiler, **kw),
               )
       
>>>>>>         if compiler is not None:
>>>>>>             toplevel = not compiler.stack
               else:
>>>>>>             toplevel = True
>>>>>>         if not toplevel:
>>>>>>             return self
       
>>>>>>         mapper = statement._propagate_attrs["plugin_subject"]
>>>>>>         dml_strategy = statement._annotations.get("dml_strategy", "raw")
>>>>>>         if dml_strategy == "bulk":
>>>>>>             self._setup_for_bulk_insert(compiler)
>>>>>>         elif dml_strategy == "orm":
>>>>>>             self._setup_for_orm_insert(compiler, mapper)
       
>>>>>>         return self
       
    1:     @classmethod
    1:     def _resolved_keys_as_col_keys(cls, mapper, resolved_value_dict):
>>>>>>         return {
>>>>>>             col.key if col is not None else k: v
>>>>>>             for col, k, v in (
>>>>>>                 (mapper.c.get(k), k, v) for k, v in resolved_value_dict.items()
                   )
               }
       
    1:     def _setup_for_orm_insert(self, compiler, mapper):
>>>>>>         statement = orm_level_statement = cast(dml.Insert, self.statement)
       
>>>>>>         statement = self._setup_orm_returning(
>>>>>>             compiler,
>>>>>>             orm_level_statement,
>>>>>>             statement,
>>>>>>             dml_mapper=mapper,
>>>>>>             use_supplemental_cols=False,
               )
>>>>>>         self.statement = statement
       
    1:     def _setup_for_bulk_insert(self, compiler):
               """establish an INSERT statement within the context of
               bulk insert.
       
               This method will be within the "conn.execute()" call that is invoked
               by persistence._emit_insert_statement().
       
               """
>>>>>>         statement = orm_level_statement = cast(dml.Insert, self.statement)
>>>>>>         an = statement._annotations
       
>>>>>>         emit_insert_table, emit_insert_mapper = (
>>>>>>             an["_emit_insert_table"],
>>>>>>             an["_emit_insert_mapper"],
               )
       
>>>>>>         statement = statement._clone()
       
>>>>>>         statement.table = emit_insert_table
>>>>>>         if self._dict_parameters:
>>>>>>             self._dict_parameters = {
>>>>>>                 col: val
>>>>>>                 for col, val in self._dict_parameters.items()
>>>>>>                 if col.table is emit_insert_table
                   }
       
>>>>>>         statement = self._setup_orm_returning(
>>>>>>             compiler,
>>>>>>             orm_level_statement,
>>>>>>             statement,
>>>>>>             dml_mapper=emit_insert_mapper,
>>>>>>             use_supplemental_cols=True,
               )
       
>>>>>>         if (
>>>>>>             self.from_statement_ctx is not None
>>>>>>             and self.from_statement_ctx.compile_options._is_star
               ):
>>>>>>             raise sa_exc.CompileError(
>>>>>>                 "Can't use RETURNING * with bulk ORM INSERT.  "
                       "Please use a different INSERT form, such as INSERT..VALUES "
                       "or INSERT with a Core Connection"
                   )
       
>>>>>>         self.statement = statement
       
       
    2: @CompileState.plugin_for("orm", "update")
    1: class BulkORMUpdate(BulkUDCompileState, UpdateDMLState):
    1:     @classmethod
    1:     def create_for_statement(cls, statement, compiler, **kw):
    1:         self = cls.__new__(cls)
       
    2:         dml_strategy = statement._annotations.get(
    1:             "dml_strategy", "unspecified"
               )
       
    1:         toplevel = not compiler.stack
       
    1:         if toplevel and dml_strategy == "bulk":
>>>>>>             self._setup_for_bulk_update(statement, compiler)
    2:         elif (
    1:             dml_strategy == "core_only"
    1:             or dml_strategy == "unspecified"
>>>>>>             and "parententity" not in statement.table._annotations
               ):
>>>>>>             UpdateDMLState.__init__(self, statement, compiler, **kw)
    1:         elif not toplevel or dml_strategy in ("orm", "unspecified"):
    1:             self._setup_for_orm_update(statement, compiler)
       
    1:         return self
       
    1:     def _setup_for_orm_update(self, statement, compiler, **kw):
    1:         orm_level_statement = statement
       
    1:         toplevel = not compiler.stack
       
    1:         ext_info = statement.table._annotations["parententity"]
       
    1:         self.mapper = mapper = ext_info.mapper
       
    1:         self._resolved_values = self._get_resolved_values(mapper, statement)
       
    2:         self._init_global_attributes(
    1:             statement,
    1:             compiler,
    1:             toplevel=toplevel,
    1:             process_criteria_for_toplevel=toplevel,
               )
       
    1:         if statement._values:
    1:             self._resolved_values = dict(self._resolved_values)
       
    1:         new_stmt = statement._clone()
       
               # note if the statement has _multi_values, these
               # are passed through to the new statement, which will then raise
               # InvalidRequestError because UPDATE doesn't support multi_values
               # right now.
    1:         if statement._ordered_values:
>>>>>>             new_stmt._ordered_values = self._resolved_values
    1:         elif statement._values:
    1:             new_stmt._values = self._resolved_values
       
    2:         new_crit = self._adjust_for_extra_criteria(
    1:             self.global_attributes, mapper
               )
    1:         if new_crit:
>>>>>>             new_stmt = new_stmt.where(*new_crit)
       
               # if we are against a lambda statement we might not be the
               # topmost object that received per-execute annotations
       
               # do this first as we need to determine if there is
               # UPDATE..FROM
       
    1:         UpdateDMLState.__init__(self, new_stmt, compiler, **kw)
       
    1:         use_supplemental_cols = False
       
    1:         if not toplevel:
>>>>>>             synchronize_session = None
               else:
    2:             synchronize_session = compiler._annotations.get(
    1:                 "synchronize_session", None
                   )
    2:         can_use_returning = compiler._annotations.get(
    1:             "can_use_returning", None
               )
    1:         if can_use_returning is not False:
                   # even though pre_exec has determined basic
                   # can_use_returning for the dialect, if we are to use
                   # RETURNING we need to run can_use_returning() at this level
                   # unconditionally because is_delete_using was not known
                   # at the pre_exec level
>>>>>>             can_use_returning = (
>>>>>>                 synchronize_session == "fetch"
>>>>>>                 and self.can_use_returning(
>>>>>>                     compiler.dialect, mapper, is_multitable=self.is_multitable
                       )
                   )
       
    1:         if synchronize_session == "fetch" and can_use_returning:
>>>>>>             use_supplemental_cols = True
       
                   # NOTE: we might want to RETURNING the actual columns to be
                   # synchronized also.  however this is complicated and difficult
                   # to align against the behavior of "evaluate".  Additionally,
                   # in a large number (if not the majority) of cases, we have the
                   # "evaluate" answer, usually a fixed value, in memory already and
                   # there's no need to re-fetch the same value
                   # over and over again.   so perhaps if it could be RETURNING just
                   # the elements that were based on a SQL expression and not
                   # a constant.   For now it doesn't quite seem worth it
>>>>>>             new_stmt = new_stmt.return_defaults(*new_stmt.table.primary_key)
       
    1:         if toplevel:
    2:             new_stmt = self._setup_orm_returning(
    1:                 compiler,
    1:                 orm_level_statement,
    1:                 new_stmt,
    1:                 dml_mapper=mapper,
    1:                 use_supplemental_cols=use_supplemental_cols,
                   )
       
    1:         self.statement = new_stmt
       
    1:     def _setup_for_bulk_update(self, statement, compiler, **kw):
               """establish an UPDATE statement within the context of
               bulk insert.
       
               This method will be within the "conn.execute()" call that is invoked
               by persistence._emit_update_statement().
       
               """
>>>>>>         statement = cast(dml.Update, statement)
>>>>>>         an = statement._annotations
       
>>>>>>         emit_update_table, _ = (
>>>>>>             an["_emit_update_table"],
>>>>>>             an["_emit_update_mapper"],
               )
       
>>>>>>         statement = statement._clone()
>>>>>>         statement.table = emit_update_table
       
>>>>>>         UpdateDMLState.__init__(self, statement, compiler, **kw)
       
>>>>>>         if self._ordered_values:
>>>>>>             raise sa_exc.InvalidRequestError(
>>>>>>                 "bulk ORM UPDATE does not support ordered_values() for "
                       "custom UPDATE statements with bulk parameter sets.  Use a "
                       "non-bulk UPDATE statement or use values()."
                   )
       
>>>>>>         if self._dict_parameters:
>>>>>>             self._dict_parameters = {
>>>>>>                 col: val
>>>>>>                 for col, val in self._dict_parameters.items()
>>>>>>                 if col.table is emit_update_table
                   }
>>>>>>         self.statement = statement
       
    1:     @classmethod
    1:     def orm_execute_statement(
               cls,
               session: Session,
               statement: dml.Update,
               params: _CoreAnyExecuteParams,
               execution_options: OrmExecuteOptionsParameter,
               bind_arguments: _BindArguments,
               conn: Connection,
           ) -> _result.Result:
    6:         update_options = execution_options.get(
    3:             "_sa_orm_update_options", cls.default_update_options
               )
       
    3:         if update_options._dml_strategy not in (
                   "orm",
                   "auto",
                   "bulk",
                   "core_only",
               ):
>>>>>>             raise sa_exc.ArgumentError(
>>>>>>                 "Valid strategies for ORM UPDATE strategy "
                       "are 'orm', 'auto', 'bulk', 'core_only'"
                   )
       
               result: _result.Result[Any]
       
    3:         if update_options._dml_strategy == "bulk":
>>>>>>             enable_check_rowcount = not statement._where_criteria
       
>>>>>>             assert update_options._synchronize_session != "fetch"
       
>>>>>>             if (
>>>>>>                 statement._where_criteria
>>>>>>                 and update_options._synchronize_session == "evaluate"
                   ):
>>>>>>                 raise sa_exc.InvalidRequestError(
>>>>>>                     "bulk synchronize of persistent objects not supported "
                           "when using bulk update with additional WHERE "
                           "criteria right now.  add synchronize_session=None "
                           "execution option to bypass synchronize of persistent "
                           "objects."
                       )
>>>>>>             mapper = update_options._subject_mapper
>>>>>>             assert mapper is not None
>>>>>>             assert session._transaction is not None
>>>>>>             result = _bulk_update(
>>>>>>                 mapper,
>>>>>>                 cast(
>>>>>>                     "Iterable[Dict[str, Any]]",
>>>>>>                     [params] if isinstance(params, dict) else params,
                       ),
>>>>>>                 session._transaction,
>>>>>>                 isstates=False,
>>>>>>                 update_changed_only=False,
>>>>>>                 use_orm_update_stmt=statement,
>>>>>>                 enable_check_rowcount=enable_check_rowcount,
                   )
>>>>>>             return cls.orm_setup_cursor_result(
>>>>>>                 session,
>>>>>>                 statement,
>>>>>>                 params,
>>>>>>                 execution_options,
>>>>>>                 bind_arguments,
>>>>>>                 result,
                   )
               else:
    6:             return super().orm_execute_statement(
    3:                 session,
    3:                 statement,
    3:                 params,
    3:                 execution_options,
    3:                 bind_arguments,
    3:                 conn,
                   )
       
    1:     @classmethod
    1:     def can_use_returning(
               cls,
               dialect: Dialect,
               mapper: Mapper[Any],
               *,
    1:         is_multitable: bool = False,
    1:         is_update_from: bool = False,
    1:         is_delete_using: bool = False,
    1:         is_executemany: bool = False,
           ) -> bool:
               # normal answer for "should we use RETURNING" at all.
>>>>>>         normal_answer = (
>>>>>>             dialect.update_returning and mapper.local_table.implicit_returning
               )
>>>>>>         if not normal_answer:
>>>>>>             return False
       
>>>>>>         if is_executemany:
>>>>>>             return dialect.update_executemany_returning
       
               # these workarounds are currently hypothetical for UPDATE,
               # unlike DELETE where they impact MariaDB
>>>>>>         if is_update_from:
>>>>>>             return dialect.update_returning_multifrom
       
>>>>>>         elif is_multitable and not dialect.update_returning_multifrom:
>>>>>>             raise sa_exc.CompileError(
>>>>>>                 f'Dialect "{dialect.name}" does not support RETURNING '
                       "with UPDATE..FROM; for synchronize_session='fetch', "
                       "please add the additional execution option "
                       "'is_update_from=True' to the statement to indicate that "
                       "a separate SELECT should be used for this backend."
                   )
       
>>>>>>         return True
       
    1:     @classmethod
    1:     def _do_post_synchronize_bulk_evaluate(
               cls, session, params, result, update_options
           ):
>>>>>>         if not params:
>>>>>>             return
       
>>>>>>         mapper = update_options._subject_mapper
>>>>>>         pk_keys = [prop.key for prop in mapper._identity_key_props]
       
>>>>>>         identity_map = session.identity_map
       
>>>>>>         for param in params:
>>>>>>             identity_key = mapper.identity_key_from_primary_key(
>>>>>>                 (param[key] for key in pk_keys),
>>>>>>                 update_options._identity_token,
                   )
>>>>>>             state = identity_map.fast_get_state(identity_key)
>>>>>>             if not state:
>>>>>>                 continue
       
>>>>>>             evaluated_keys = set(param).difference(pk_keys)
       
>>>>>>             dict_ = state.dict
                   # only evaluate unmodified attributes
>>>>>>             to_evaluate = state.unmodified.intersection(evaluated_keys)
>>>>>>             for key in to_evaluate:
>>>>>>                 if key in dict_:
>>>>>>                     dict_[key] = param[key]
       
>>>>>>             state.manager.dispatch.refresh(state, None, to_evaluate)
       
>>>>>>             state._commit(dict_, list(to_evaluate))
       
                   # attributes that were formerly modified instead get expired.
                   # this only gets hit if the session had pending changes
                   # and autoflush were set to False.
>>>>>>             to_expire = evaluated_keys.intersection(dict_).difference(
>>>>>>                 to_evaluate
                   )
>>>>>>             if to_expire:
>>>>>>                 state._expire_attributes(dict_, to_expire)
       
    1:     @classmethod
    1:     def _do_post_synchronize_evaluate(
               cls, session, statement, result, update_options
           ):
    6:         matched_objects = cls._get_matched_objects_on_criteria(
    3:             update_options,
    3:             session.identity_map.all_states(),
               )
       
    6:         cls._apply_update_set_values_to_objects(
    3:             session,
    3:             update_options,
    3:             statement,
    7:             [(obj, state, dict_) for obj, state, dict_, _ in matched_objects],
               )
       
    1:     @classmethod
    1:     def _do_post_synchronize_fetch(
               cls, session, statement, result, update_options
           ):
>>>>>>         target_mapper = update_options._subject_mapper
       
>>>>>>         returned_defaults_rows = result.returned_defaults_rows
>>>>>>         if returned_defaults_rows:
>>>>>>             pk_rows = cls._interpret_returning_rows(
>>>>>>                 target_mapper, returned_defaults_rows
                   )
       
>>>>>>             matched_rows = [
>>>>>>                 tuple(row) + (update_options._identity_token,)
>>>>>>                 for row in pk_rows
                   ]
               else:
>>>>>>             matched_rows = update_options._matched_rows
       
>>>>>>         objs = [
>>>>>>             session.identity_map[identity_key]
>>>>>>             for identity_key in [
>>>>>>                 target_mapper.identity_key_from_primary_key(
>>>>>>                     list(primary_key),
>>>>>>                     identity_token=identity_token,
                       )
>>>>>>                 for primary_key, identity_token in [
>>>>>>                     (row[0:-1], row[-1]) for row in matched_rows
                       ]
>>>>>>                 if update_options._identity_token is None
>>>>>>                 or identity_token == update_options._identity_token
                   ]
>>>>>>             if identity_key in session.identity_map
               ]
       
>>>>>>         if not objs:
>>>>>>             return
       
>>>>>>         cls._apply_update_set_values_to_objects(
>>>>>>             session,
>>>>>>             update_options,
>>>>>>             statement,
>>>>>>             [
>>>>>>                 (
>>>>>>                     obj,
>>>>>>                     attributes.instance_state(obj),
>>>>>>                     attributes.instance_dict(obj),
                       )
>>>>>>                 for obj in objs
                   ],
               )
       
    1:     @classmethod
    1:     def _apply_update_set_values_to_objects(
               cls, session, update_options, statement, matched_objects
           ):
               """apply values to objects derived from an update statement, e.g.
               UPDATE..SET <values>
       
               """
    3:         mapper = update_options._subject_mapper
    3:         target_cls = mapper.class_
    3:         evaluator_compiler = evaluator._EvaluatorCompiler(target_cls)
    3:         resolved_values = cls._get_resolved_values(mapper, statement)
    6:         resolved_keys_as_propnames = cls._resolved_keys_as_propnames(
    3:             mapper, resolved_values
               )
    3:         value_evaluators = {}
    6:         for key, value in resolved_keys_as_propnames:
    3:             try:
    6:                 _evaluator = evaluator_compiler.process(
    3:                     coercions.expect(roles.ExpressionElementRole, value)
                       )
>>>>>>             except evaluator.UnevaluatableError:
>>>>>>                 pass
                   else:
    3:                 value_evaluators[key] = _evaluator
       
    3:         evaluated_keys = list(value_evaluators.keys())
    9:         attrib = {k for k, v in resolved_keys_as_propnames}
       
    3:         states = set()
    4:         for obj, state, dict_ in matched_objects:
    1:             to_evaluate = state.unmodified.intersection(evaluated_keys)
       
    2:             for key in to_evaluate:
    1:                 if key in dict_:
                           # only run eval for attributes that are present.
    1:                     dict_[key] = value_evaluators[key](obj)
       
    1:             state.manager.dispatch.refresh(state, None, to_evaluate)
       
    1:             state._commit(dict_, list(to_evaluate))
       
                   # attributes that were formerly modified instead get expired.
                   # this only gets hit if the session had pending changes
                   # and autoflush were set to False.
    1:             to_expire = attrib.intersection(dict_).difference(to_evaluate)
    1:             if to_expire:
>>>>>>                 state._expire_attributes(dict_, to_expire)
       
    1:             states.add(state)
    3:         session._register_altered(states)
       
       
    2: @CompileState.plugin_for("orm", "delete")
    1: class BulkORMDelete(BulkUDCompileState, DeleteDMLState):
    1:     @classmethod
    1:     def create_for_statement(cls, statement, compiler, **kw):
    1:         self = cls.__new__(cls)
       
    2:         dml_strategy = statement._annotations.get(
    1:             "dml_strategy", "unspecified"
               )
       
    2:         if (
    1:             dml_strategy == "core_only"
    1:             or dml_strategy == "unspecified"
>>>>>>             and "parententity" not in statement.table._annotations
               ):
>>>>>>             DeleteDMLState.__init__(self, statement, compiler, **kw)
>>>>>>             return self
       
    1:         toplevel = not compiler.stack
       
    1:         orm_level_statement = statement
       
    1:         ext_info = statement.table._annotations["parententity"]
    1:         self.mapper = mapper = ext_info.mapper
       
    2:         self._init_global_attributes(
    1:             statement,
    1:             compiler,
    1:             toplevel=toplevel,
    1:             process_criteria_for_toplevel=toplevel,
               )
       
    1:         new_stmt = statement._clone()
       
    2:         new_crit = cls._adjust_for_extra_criteria(
    1:             self.global_attributes, mapper
               )
    1:         if new_crit:
>>>>>>             new_stmt = new_stmt.where(*new_crit)
       
               # do this first as we need to determine if there is
               # DELETE..FROM
    1:         DeleteDMLState.__init__(self, new_stmt, compiler, **kw)
       
    1:         use_supplemental_cols = False
       
    1:         if not toplevel:
>>>>>>             synchronize_session = None
               else:
    2:             synchronize_session = compiler._annotations.get(
    1:                 "synchronize_session", None
                   )
    2:         can_use_returning = compiler._annotations.get(
    1:             "can_use_returning", None
               )
    1:         if can_use_returning is not False:
                   # even though pre_exec has determined basic
                   # can_use_returning for the dialect, if we are to use
                   # RETURNING we need to run can_use_returning() at this level
                   # unconditionally because is_delete_using was not known
                   # at the pre_exec level
>>>>>>             can_use_returning = (
>>>>>>                 synchronize_session == "fetch"
>>>>>>                 and self.can_use_returning(
>>>>>>                     compiler.dialect,
>>>>>>                     mapper,
>>>>>>                     is_multitable=self.is_multitable,
>>>>>>                     is_delete_using=compiler._annotations.get(
>>>>>>                         "is_delete_using", False
                           ),
                       )
                   )
       
    1:         if can_use_returning:
>>>>>>             use_supplemental_cols = True
       
>>>>>>             new_stmt = new_stmt.return_defaults(*new_stmt.table.primary_key)
       
    1:         if toplevel:
    2:             new_stmt = self._setup_orm_returning(
    1:                 compiler,
    1:                 orm_level_statement,
    1:                 new_stmt,
    1:                 dml_mapper=mapper,
    1:                 use_supplemental_cols=use_supplemental_cols,
                   )
       
    1:         self.statement = new_stmt
       
    1:         return self
       
    1:     @classmethod
    1:     def orm_execute_statement(
               cls,
               session: Session,
               statement: dml.Delete,
               params: _CoreAnyExecuteParams,
               execution_options: OrmExecuteOptionsParameter,
               bind_arguments: _BindArguments,
               conn: Connection,
           ) -> _result.Result:
    4:         update_options = execution_options.get(
    2:             "_sa_orm_update_options", cls.default_update_options
               )
       
    2:         if update_options._dml_strategy == "bulk":
>>>>>>             raise sa_exc.InvalidRequestError(
>>>>>>                 "Bulk ORM DELETE not supported right now. "
                       "Statement may be invoked at the "
                       "Core level using "
                       "session.connection().execute(stmt, parameters)"
                   )
       
    2:         if update_options._dml_strategy not in ("orm", "auto", "core_only"):
>>>>>>             raise sa_exc.ArgumentError(
>>>>>>                 "Valid strategies for ORM DELETE strategy are 'orm', 'auto', "
                       "'core_only'"
                   )
       
    4:         return super().orm_execute_statement(
    2:             session, statement, params, execution_options, bind_arguments, conn
               )
       
    1:     @classmethod
    1:     def can_use_returning(
               cls,
               dialect: Dialect,
               mapper: Mapper[Any],
               *,
    1:         is_multitable: bool = False,
    1:         is_update_from: bool = False,
    1:         is_delete_using: bool = False,
    1:         is_executemany: bool = False,
           ) -> bool:
               # normal answer for "should we use RETURNING" at all.
>>>>>>         normal_answer = (
>>>>>>             dialect.delete_returning and mapper.local_table.implicit_returning
               )
>>>>>>         if not normal_answer:
>>>>>>             return False
       
               # now get into special workarounds because MariaDB supports
               # DELETE...RETURNING but not DELETE...USING...RETURNING.
>>>>>>         if is_delete_using:
                   # is_delete_using hint was passed.   use
                   # additional dialect feature (True for PG, False for MariaDB)
>>>>>>             return dialect.delete_returning_multifrom
       
>>>>>>         elif is_multitable and not dialect.delete_returning_multifrom:
                   # is_delete_using hint was not passed, but we determined
                   # at compile time that this is in fact a DELETE..USING.
                   # it's too late to continue since we did not pre-SELECT.
                   # raise that we need that hint up front.
       
>>>>>>             raise sa_exc.CompileError(
>>>>>>                 f'Dialect "{dialect.name}" does not support RETURNING '
                       "with DELETE..USING; for synchronize_session='fetch', "
                       "please add the additional execution option "
                       "'is_delete_using=True' to the statement to indicate that "
                       "a separate SELECT should be used for this backend."
                   )
       
>>>>>>         return True
       
    1:     @classmethod
    1:     def _do_post_synchronize_evaluate(
               cls, session, statement, result, update_options
           ):
>>>>>>         matched_objects = cls._get_matched_objects_on_criteria(
>>>>>>             update_options,
>>>>>>             session.identity_map.all_states(),
               )
       
>>>>>>         to_delete = []
       
>>>>>>         for _, state, dict_, is_partially_expired in matched_objects:
>>>>>>             if is_partially_expired:
>>>>>>                 state._expire(dict_, session.identity_map._modified)
                   else:
>>>>>>                 to_delete.append(state)
       
>>>>>>         if to_delete:
>>>>>>             session._remove_newly_deleted(to_delete)
       
    1:     @classmethod
    1:     def _do_post_synchronize_fetch(
               cls, session, statement, result, update_options
           ):
>>>>>>         target_mapper = update_options._subject_mapper
       
>>>>>>         returned_defaults_rows = result.returned_defaults_rows
       
>>>>>>         if returned_defaults_rows:
>>>>>>             pk_rows = cls._interpret_returning_rows(
>>>>>>                 target_mapper, returned_defaults_rows
                   )
       
>>>>>>             matched_rows = [
>>>>>>                 tuple(row) + (update_options._identity_token,)
>>>>>>                 for row in pk_rows
                   ]
               else:
>>>>>>             matched_rows = update_options._matched_rows
       
>>>>>>         for row in matched_rows:
>>>>>>             primary_key = row[0:-1]
>>>>>>             identity_token = row[-1]
       
                   # TODO: inline this and call remove_newly_deleted
                   # once
>>>>>>             identity_key = target_mapper.identity_key_from_primary_key(
>>>>>>                 list(primary_key),
>>>>>>                 identity_token=identity_token,
                   )
>>>>>>             if identity_key in session.identity_map:
>>>>>>                 session._remove_newly_deleted(
>>>>>>                     [
>>>>>>                         attributes.instance_state(
>>>>>>                             session.identity_map[identity_key]
                               )
                           ]
                       )
